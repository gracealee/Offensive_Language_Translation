{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W12HsiUMZ486",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf377f5-6e1e-477b-bcca-470fe883a1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.5 MB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 73.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 62.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 90.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 90.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 89.5 MB/s \n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install these packages if running from colab\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install pydot --quiet\n",
        "!pip install transformers --quiet\n",
        "\n",
        "# install huggingface datasets\n",
        "!pip install datasets --quiet\n",
        "\n",
        "! pip install rouge-score nltk --quiet\n",
        "! pip install huggingface_hub --quiet\n",
        "\n",
        "!pip install sentencepiece --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "#let's make longer output readable without scrolling\n",
        "from pprint import pprint\n",
        "\n",
        "# the toxic parallel dataset, with rouge metric\n",
        "from datasets import load_dataset, load_from_disk, load_metric, DatasetDict"
      ],
      "metadata": {
        "id": "mcBX4XFjaAcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OZu1YOSiawXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f31645b-454f-4be7-ba5c-a91eea891ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define paths\n",
        "csv_path = 'drive/MyDrive/Colab Notebooks/w266_project_predictions/'"
      ],
      "metadata": {
        "id": "9WUSQ0yZaAf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Change these variables as needed for different model and different file name"
      ],
      "metadata": {
        "id": "4G3BLdt-oaHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change these variables for different models\n",
        "output_file_name = 'davidson_paradetox_output.csv'"
      ],
      "metadata": {
        "id": "XnU-wafMoPyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load BART Base Pretrain + paraDetox Fine-Tuned model"
      ],
      "metadata": {
        "id": "yzu9cmNzipvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using detox pretrained\n",
        "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
        "base_model_name = 'facebook/bart-base'\n",
        "model_name = 'SkolkovoInstitute/bart-base-detox'\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "wVIB6m0y3PRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try_phrase = 'brah im fucked up over here .'\n",
        "input_tokenized = tokenizer([try_phrase], return_tensors=\"pt\").input_ids\n",
        "summary_ids = model.generate(input_tokenized, num_beams=2, min_length=0, max_length=65)\n",
        "\n",
        "prediction = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4DbFc_FurtF",
        "outputId": "9a9c7259-2383-44c7-f765-c6fb67a96ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brah im messed up over here .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Davison Dataset"
      ],
      "metadata": {
        "id": "9aHayzabrMMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv'\n",
        "dataset = pd.read_csv(url, index_col=0)\n",
        "df = dataset"
      ],
      "metadata": {
        "id": "2CL1vy0ArK-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the initial exclamation points and the RT twitter handles\n",
        "df['tweet'] = df['tweet'].apply(lambda x: \": \".join(x.split(\": \")[1:]) if len(x.split(\": \")) > 1 else x)\n",
        "# remove the unicode symbols \n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(\"&#\\d+\",\"\",x))\n",
        "\n",
        "# remove other @handles \n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(\"@[^ ]+ \",\"\",x))\n",
        "df['tweet']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8xTSZucMn_1",
        "outputId": "214b4a66-0bdd-4b2b-cf86-065d734dd14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        As a woman you shouldn't complain about cleani...\n",
              "1        boy dats cold...tyga dwn bad for cuffin dat ho...\n",
              "2        You ever fuck a bitch and she start to cry? Yo...\n",
              "3                                   she look like a tranny\n",
              "4        The shit you hear about me might be true or it...\n",
              "                               ...                        \n",
              "25291    right! His TL is trash ;. Now, mine? Bible scr...\n",
              "25292    you've gone and broke the wrong heart baby, an...\n",
              "25294    young buck wanna eat!!.. dat nigguh like I ain...\n",
              "25295                youu got wild bitches tellin you lies\n",
              "25296    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
              "Name: tweet, Length: 24783, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_tweets, testing_tweets = train_test_split(df, test_size=0.2, random_state=25, stratify = df['class'])\n",
        "valid_tweets, testing_tweets = train_test_split(testing_tweets, test_size = 0.5, random_state=25, stratify = testing_tweets['class'])"
      ],
      "metadata": {
        "id": "QB3XA57qNOgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"No. of training examples: {training_tweets.shape[0]}\")\n",
        "print(f\"No. of validation examples: {valid_tweets.shape[0]}\")\n",
        "print(f\"No. of testing examples: {testing_tweets.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdPJBrjeNQQk",
        "outputId": "f46f42e9-f43a-4e03-d130-2d192480c734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples: 19826\n",
            "No. of validation examples: 2478\n",
            "No. of testing examples: 2479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.generate() to a CSV file"
      ],
      "metadata": {
        "id": "kMPfHq9trRCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "V7Qi04SLBP-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_tweets = training_tweets['tweet']\n",
        "valid_tweets = valid_tweets['tweet']\n",
        "testing_tweets = testing_tweets['tweet']"
      ],
      "metadata": {
        "id": "05NaDYfLrAoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FZAEA7NQjlq",
        "outputId": "fcffc131-355e-4c1c-8198-bb3844d56336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "601      \"Why would you wanna be the Green Ranger? He's...\n",
              "2353     #HolySpirit God still share HIS #Secrets Amos ...\n",
              "24847                                       pancakes trash\n",
              "21958    The KFAN mock draft continues, Cleveland is \"o...\n",
              "10327    I be telling Mcgirt music ain't enough.You got...\n",
              "                               ...                        \n",
              "21327     Slack jawed yokel husband http://t.co/VE1PWFrz9t\n",
              "8898     Dating you would be like Darnell dating that f...\n",
              "3744     Did you say spray tan? **Charlie Crist switche...\n",
              "24157    bitches be like \" I'm a squirter\" but thinkin ...\n",
              "24013           Zack still questions my love for Oreos lol\n",
              "Name: tweet, Length: 2479, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "predictions = []\n",
        "curr_df = testing_tweets\n",
        "length = len(curr_df)\n",
        "batch_size = 10\n",
        "\n",
        "for i in range(int(length/batch_size)):\n",
        "  start_time = time.time()\n",
        "  list_start = int(i*batch_size)\n",
        "  list_end = int((i+1)*batch_size)\n",
        "  if (int(i+1)*batch_size > length):\n",
        "    list_end = length-1\n",
        "\n",
        "  input_tokenized = tokenizer(list(curr_df[list_start:list_end]), return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
        "  summary_ids = model.generate(input_tokenized, num_beams=2, min_length=0, max_length=65)\n",
        "  \n",
        "  prediction = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "  input = curr_df[list_start:list_end]\n",
        "  \n",
        "  predictions.extend(prediction)\n",
        "  inputs.extend(input)\n",
        "\n",
        "  end_time = time.time()\n",
        "  print('complete', i*batch_size, '/', length, ': ', end_time - start_time, 'in this batch.')\n",
        "#print(len(val_references))"
      ],
      "metadata": {
        "id": "oFRJNDv0rY1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d452ea46-02d2-430f-ad3c-4d019edff691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete 0 / 2479 :  230.86832809448242 in this batch.\n",
            "complete 200 / 2479 :  279.84443736076355 in this batch.\n",
            "complete 400 / 2479 :  278.94853472709656 in this batch.\n",
            "complete 600 / 2479 :  257.8484380245209 in this batch.\n",
            "complete 800 / 2479 :  272.0145535469055 in this batch.\n",
            "complete 1000 / 2479 :  313.78338980674744 in this batch.\n",
            "complete 1200 / 2479 :  293.7940146923065 in this batch.\n",
            "complete 1400 / 2479 :  293.3756878376007 in this batch.\n",
            "complete 1600 / 2479 :  279.6716446876526 in this batch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "L2L6Qb-8qxCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "1eRdrcA9q3ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict = {'train_inputs': inputs, 'train_predictions': predictions}  \n",
        "       \n",
        "df = pd.DataFrame(dict) "
      ],
      "metadata": {
        "id": "uQcaycTB4jhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "crNUd2HlqQm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # saving the output dataframe to a csv file\n",
        "df.to_csv(csv_path + output_file_name, index = False) "
      ],
      "metadata": {
        "id": "egVKDxk54k--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mqncpOSyFMPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}