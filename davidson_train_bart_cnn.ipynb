{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W12HsiUMZ486"
      },
      "outputs": [],
      "source": [
        "# Install these packages if running from colab\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install pydot --quiet\n",
        "!pip install transformers --quiet\n",
        "\n",
        "# install huggingface datasets\n",
        "!pip install datasets --quiet\n",
        "\n",
        "! pip install rouge-score nltk --quiet\n",
        "! pip install huggingface_hub --quiet\n",
        "\n",
        "!pip install sentencepiece --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "#let's make longer output readable without scrolling\n",
        "from pprint import pprint\n",
        "\n",
        "# the toxic parallel dataset, with rouge metric\n",
        "from datasets import load_dataset, load_from_disk, load_metric, DatasetDict"
      ],
      "metadata": {
        "id": "mcBX4XFjaAcw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the Drive helper and mount\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OZu1YOSiawXI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define paths\n",
        "csv_path = 'w266_project_predictions/'\n",
        "model_path = 'w266_project_models/'"
      ],
      "metadata": {
        "id": "9WUSQ0yZaAf0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Davison Dataset"
      ],
      "metadata": {
        "id": "9aHayzabrMMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv'\n",
        "dataset = pd.read_csv(url, index_col=0)\n",
        "df = dataset"
      ],
      "metadata": {
        "id": "2CL1vy0ArK-9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the initial exclamation points and the RT twitter handles\n",
        "df['tweet'] = df['tweet'].apply(lambda x: \": \".join(x.split(\": \")[1:]) if len(x.split(\": \")) > 1 else x)\n",
        "# remove the unicode symbols \n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(\"&#\\d+\",\"\",x))\n",
        "\n",
        "# remove other @handles \n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(\"@[^ ]+ \",\"\",x))\n",
        "df['tweet']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8xTSZucMn_1",
        "outputId": "41c9a3e7-38b2-44bd-9264-ed2fa3d53b1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        As a woman you shouldn't complain about cleani...\n",
              "1        boy dats cold...tyga dwn bad for cuffin dat ho...\n",
              "2        You ever fuck a bitch and she start to cry? Yo...\n",
              "3                                   she look like a tranny\n",
              "4        The shit you hear about me might be true or it...\n",
              "                               ...                        \n",
              "25291    right! His TL is trash ;. Now, mine? Bible scr...\n",
              "25292    you've gone and broke the wrong heart baby, an...\n",
              "25294    young buck wanna eat!!.. dat nigguh like I ain...\n",
              "25295                youu got wild bitches tellin you lies\n",
              "25296    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
              "Name: tweet, Length: 24783, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_tweets, testing_tweets = train_test_split(df, test_size=0.2, random_state=25, stratify = df['class'])\n",
        "valid_tweets, testing_tweets = train_test_split(testing_tweets, test_size = 0.5, random_state=25, stratify = testing_tweets['class'])"
      ],
      "metadata": {
        "id": "QB3XA57qNOgp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"No. of training examples: {training_tweets.shape[0]}\")\n",
        "print(f\"No. of validation examples: {valid_tweets.shape[0]}\")\n",
        "print(f\"No. of testing examples: {testing_tweets.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdPJBrjeNQQk",
        "outputId": "63754393-ee21-489c-d7f2-8f36be1ba458"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples: 19826\n",
            "No. of validation examples: 2478\n",
            "No. of testing examples: 2479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.generate() to a CSV file"
      ],
      "metadata": {
        "id": "kMPfHq9trRCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"facebook/bart-large-cnn\"\n",
        "model_name = 'bart_cnn_weights.hdf5'"
      ],
      "metadata": {
        "id": "gNfpblcnDiUL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, TFBartForConditionalGeneration\n",
        "\n",
        "bart_cnn_model = TFBartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "HC7YcP6BEQfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a93f02-f006-49b3-e11c-6fd22ef74e64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 65"
      ],
      "metadata": {
        "id": "j3bat1GHdJ-j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_file = model_path + model_name\n",
        "bart_cnn_model.load_weights(checkpoint_file)"
      ],
      "metadata": {
        "id": "B6NfgX3sERKX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "V7Qi04SLBP-F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_tweets['tweet']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FZAEA7NQjlq",
        "outputId": "b2648bf4-8329-4a0c-c69d-135e2b080471"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "601      \"Why would you wanna be the Green Ranger? He's...\n",
              "2353     #HolySpirit God still share HIS #Secrets Amos ...\n",
              "24847                                       pancakes trash\n",
              "21958    The KFAN mock draft continues, Cleveland is \"o...\n",
              "10327    I be telling Mcgirt music ain't enough.You got...\n",
              "                               ...                        \n",
              "21327     Slack jawed yokel husband http://t.co/VE1PWFrz9t\n",
              "8898     Dating you would be like Darnell dating that f...\n",
              "3744     Did you say spray tan? **Charlie Crist switche...\n",
              "24157    bitches be like \" I'm a squirter\" but thinkin ...\n",
              "24013           Zack still questions my love for Oreos lol\n",
              "Name: tweet, Length: 2479, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "predictions = []\n",
        "curr_df = testing_tweets['tweet']\n",
        "length = len(curr_df)\n",
        "batch_size = 10\n",
        "\n",
        "for i in range(int(length/batch_size)):\n",
        "  start_time = time.time()\n",
        "  list_start = int(i*batch_size)\n",
        "  list_end = int((i+1)*batch_size)\n",
        "  if (int(i+1)*batch_size > length):\n",
        "    list_end = length-1\n",
        "\n",
        "  input_tokenized = bart_tokenizer(list(curr_df[list_start:list_end]), return_tensors=\"tf\",padding=True, truncation=True).input_ids\n",
        "  summary_ids = bart_cnn_model.generate(input_tokenized, num_beams=2, min_length=0, max_length=max_length)\n",
        "  \n",
        "  prediction = bart_tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "  input = curr_df[list_start:list_end]\n",
        "  \n",
        "  predictions.extend(prediction)\n",
        "  inputs.extend(input)\n",
        "\n",
        "  if i % 2 == 0:\n",
        "    end_time = time.time()\n",
        "    print('complete', i*batch_size, '/', length, ': ', end_time - start_time, 'per record.')\n",
        "#print(len(val_references))"
      ],
      "metadata": {
        "id": "FgROdqB0EqZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1a3773-0096-470f-b7f6-6810416cd45d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete 0 / 2479 :  17.4843008518219 per record.\n",
            "complete 20 / 2479 :  20.20796489715576 per record.\n",
            "complete 40 / 2479 :  16.9012348651886 per record.\n",
            "complete 60 / 2479 :  18.953460454940796 per record.\n",
            "complete 80 / 2479 :  20.6339693069458 per record.\n",
            "complete 100 / 2479 :  20.221646785736084 per record.\n",
            "complete 120 / 2479 :  18.494943618774414 per record.\n",
            "complete 140 / 2479 :  18.132465839385986 per record.\n",
            "complete 160 / 2479 :  17.706299304962158 per record.\n",
            "complete 180 / 2479 :  19.654677629470825 per record.\n",
            "complete 200 / 2479 :  21.192508935928345 per record.\n",
            "complete 220 / 2479 :  24.498835563659668 per record.\n",
            "complete 240 / 2479 :  13.387699127197266 per record.\n",
            "complete 260 / 2479 :  17.44926428794861 per record.\n",
            "complete 280 / 2479 :  15.749465465545654 per record.\n",
            "complete 300 / 2479 :  18.91218376159668 per record.\n",
            "complete 320 / 2479 :  19.147905826568604 per record.\n",
            "complete 340 / 2479 :  19.047999620437622 per record.\n",
            "complete 360 / 2479 :  17.871283531188965 per record.\n",
            "complete 380 / 2479 :  18.534205436706543 per record.\n",
            "complete 400 / 2479 :  21.329045295715332 per record.\n",
            "complete 420 / 2479 :  17.8117573261261 per record.\n",
            "complete 440 / 2479 :  22.79976797103882 per record.\n",
            "complete 460 / 2479 :  20.253144025802612 per record.\n",
            "complete 480 / 2479 :  17.87455129623413 per record.\n",
            "complete 500 / 2479 :  22.564289569854736 per record.\n",
            "complete 520 / 2479 :  17.563527584075928 per record.\n",
            "complete 540 / 2479 :  21.683667182922363 per record.\n",
            "complete 560 / 2479 :  19.498141765594482 per record.\n",
            "complete 580 / 2479 :  15.657402753829956 per record.\n",
            "complete 600 / 2479 :  18.919257402420044 per record.\n",
            "complete 620 / 2479 :  16.22108030319214 per record.\n",
            "complete 640 / 2479 :  20.229854822158813 per record.\n",
            "complete 660 / 2479 :  17.686158657073975 per record.\n",
            "complete 680 / 2479 :  20.62235140800476 per record.\n",
            "complete 700 / 2479 :  21.39983081817627 per record.\n",
            "complete 720 / 2479 :  18.29396915435791 per record.\n",
            "complete 740 / 2479 :  19.61830973625183 per record.\n",
            "complete 760 / 2479 :  18.537838220596313 per record.\n",
            "complete 780 / 2479 :  18.242767333984375 per record.\n",
            "complete 800 / 2479 :  18.850666522979736 per record.\n",
            "complete 820 / 2479 :  19.47758913040161 per record.\n",
            "complete 840 / 2479 :  21.544357538223267 per record.\n",
            "complete 860 / 2479 :  20.003982543945312 per record.\n",
            "complete 880 / 2479 :  20.701794862747192 per record.\n",
            "complete 900 / 2479 :  19.024906873703003 per record.\n",
            "complete 920 / 2479 :  19.690917015075684 per record.\n",
            "complete 940 / 2479 :  17.850805044174194 per record.\n",
            "complete 960 / 2479 :  16.66512703895569 per record.\n",
            "complete 980 / 2479 :  20.412319660186768 per record.\n",
            "complete 1000 / 2479 :  17.67516303062439 per record.\n",
            "complete 1020 / 2479 :  18.4209623336792 per record.\n",
            "complete 1040 / 2479 :  16.735012769699097 per record.\n",
            "complete 1060 / 2479 :  18.838978052139282 per record.\n",
            "complete 1080 / 2479 :  19.56085228919983 per record.\n",
            "complete 1100 / 2479 :  17.232290029525757 per record.\n",
            "complete 1120 / 2479 :  18.994980812072754 per record.\n",
            "complete 1140 / 2479 :  19.298959970474243 per record.\n",
            "complete 1160 / 2479 :  16.78515076637268 per record.\n",
            "complete 1180 / 2479 :  18.57448673248291 per record.\n",
            "complete 1200 / 2479 :  20.29885983467102 per record.\n",
            "complete 1220 / 2479 :  16.50117826461792 per record.\n",
            "complete 1240 / 2479 :  18.53144907951355 per record.\n",
            "complete 1260 / 2479 :  19.611995458602905 per record.\n",
            "complete 1280 / 2479 :  21.58064556121826 per record.\n",
            "complete 1300 / 2479 :  18.09849786758423 per record.\n",
            "complete 1320 / 2479 :  17.55126404762268 per record.\n",
            "complete 1340 / 2479 :  21.223164796829224 per record.\n",
            "complete 1360 / 2479 :  20.071988105773926 per record.\n",
            "complete 1380 / 2479 :  17.44417095184326 per record.\n",
            "complete 1400 / 2479 :  26.06589412689209 per record.\n",
            "complete 1420 / 2479 :  17.17450261116028 per record.\n",
            "complete 1440 / 2479 :  19.937665462493896 per record.\n",
            "complete 1460 / 2479 :  19.714436769485474 per record.\n",
            "complete 1480 / 2479 :  19.61901044845581 per record.\n",
            "complete 1500 / 2479 :  22.332235097885132 per record.\n",
            "complete 1520 / 2479 :  17.956942319869995 per record.\n",
            "complete 1540 / 2479 :  17.004947900772095 per record.\n",
            "complete 1560 / 2479 :  17.639140844345093 per record.\n",
            "complete 1580 / 2479 :  19.325648069381714 per record.\n",
            "complete 1600 / 2479 :  19.03321933746338 per record.\n",
            "complete 1620 / 2479 :  19.865764379501343 per record.\n",
            "complete 1640 / 2479 :  20.759405612945557 per record.\n",
            "complete 1660 / 2479 :  17.601682424545288 per record.\n",
            "complete 1680 / 2479 :  19.18537712097168 per record.\n",
            "complete 1700 / 2479 :  16.77246880531311 per record.\n",
            "complete 1720 / 2479 :  18.257490873336792 per record.\n",
            "complete 1740 / 2479 :  16.8278751373291 per record.\n",
            "complete 1760 / 2479 :  17.43222427368164 per record.\n",
            "complete 1780 / 2479 :  16.203683853149414 per record.\n",
            "complete 1800 / 2479 :  22.65166997909546 per record.\n",
            "complete 1820 / 2479 :  16.31995415687561 per record.\n",
            "complete 1840 / 2479 :  22.93316888809204 per record.\n",
            "complete 1860 / 2479 :  21.910923719406128 per record.\n",
            "complete 1880 / 2479 :  19.037127256393433 per record.\n",
            "complete 1900 / 2479 :  20.12247085571289 per record.\n",
            "complete 1920 / 2479 :  20.767680168151855 per record.\n",
            "complete 1940 / 2479 :  15.712499380111694 per record.\n",
            "complete 1960 / 2479 :  19.590962409973145 per record.\n",
            "complete 1980 / 2479 :  21.59064292907715 per record.\n",
            "complete 2000 / 2479 :  19.282310962677002 per record.\n",
            "complete 2020 / 2479 :  25.531617641448975 per record.\n",
            "complete 2040 / 2479 :  25.616919994354248 per record.\n",
            "complete 2060 / 2479 :  18.325098276138306 per record.\n",
            "complete 2080 / 2479 :  17.4843168258667 per record.\n",
            "complete 2100 / 2479 :  23.392239093780518 per record.\n",
            "complete 2120 / 2479 :  16.336102724075317 per record.\n",
            "complete 2140 / 2479 :  17.36015009880066 per record.\n",
            "complete 2160 / 2479 :  18.538560152053833 per record.\n",
            "complete 2180 / 2479 :  18.232149600982666 per record.\n",
            "complete 2200 / 2479 :  18.0857937335968 per record.\n",
            "complete 2220 / 2479 :  18.341118812561035 per record.\n",
            "complete 2240 / 2479 :  15.249640941619873 per record.\n",
            "complete 2260 / 2479 :  16.995008945465088 per record.\n",
            "complete 2280 / 2479 :  16.52735924720764 per record.\n",
            "complete 2300 / 2479 :  20.087047576904297 per record.\n",
            "complete 2320 / 2479 :  19.8852641582489 per record.\n",
            "complete 2340 / 2479 :  19.640592098236084 per record.\n",
            "complete 2360 / 2479 :  18.51803708076477 per record.\n",
            "complete 2380 / 2479 :  19.431233882904053 per record.\n",
            "complete 2400 / 2479 :  18.739030838012695 per record.\n",
            "complete 2420 / 2479 :  18.959580183029175 per record.\n",
            "complete 2440 / 2479 :  21.612694263458252 per record.\n",
            "complete 2460 / 2479 :  17.614546060562134 per record.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict = {'test_inputs': inputs, 'test_predictions': predictions}  \n",
        "       \n",
        "df = pd.DataFrame(dict) \n",
        "# # saving the output dataframe to a csv file\n",
        "output_file_name = 'davidson_bart_cnn_test.csv'\n",
        "df.to_csv(csv_path + output_file_name, index = False) "
      ],
      "metadata": {
        "id": "JS14f0-JE_QE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mqncpOSyFMPC"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}